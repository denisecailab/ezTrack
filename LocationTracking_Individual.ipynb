{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The following code was designed in order to track the location of a single animal across the course of a session.  After initally loading in the video, the user is able to crop the video frame using a rectangle selection tool.  A background reference frame is then specified, either by taking a median of several frames in the video, or by the user providing a short video of the same environment without an animal in it.  By comparing each frame in the video to the reference frame, the location of the animal can be tracked.  It is imperative that the reference frame of the video is not shifted from the actual video.  For this reason, if a separate video is supplied, it is best that it be acquired on the same day as the behavioral recording.  The location of the animal (it's center of mass, or COM) in x,y coordinates is then recorded, as well as the distance in pixels that the animal moves from one frame to the next. Lastly, the user can specify regions of interest in the frame (e.g. left, right) using a polygon drawing tool and record for each frame whether the animal is in the region of interest.  Options for summarizing the data are also provided. \n",
    "\n",
    "### Package Requirements\n",
    "Please see instructions under repository README for package requirements and install instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Load Necessary Packages\n",
    "The following code loads neccessary packages and need not be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import LocationTracking_Functions as lt\n",
    "import holoviews as hv\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. User Sets Directory and File Information\n",
    "***Windows Users:*** Place an 'r' in front directory path (e.g. r\"zp\\Videos\") to avoid mishandling of forward slashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {\n",
    "    'dpath' : \"/Users/zachpennington/Desktop/Videos/\", # directory containing file\n",
    "    'file' : \"190306_LightDark_CPP6.wmv\", #filename with extension\n",
    "    'fps' : 30, #frames per second\n",
    "    'start' : 0, #frame at which to start. 0-based\n",
    "    'end' : None #frame at which to end.  set to None if processing whole video.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Load Video and Crop Frame if Desired\n",
    "To crop video frame, after running code below, select box selection tool below image (square with a plus sign).  To start drawing region to be included in analyis, double click image.  Double click again to finalize region.  If you decide to change region, it is best to rerun this cell and subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "#stretch image size if desired. only necessary if video width/height are odd\n",
    "#which can make image hard to see.\n",
    "stretch ={\n",
    "    'width' : 1 , #Default=1. Can be used to stretch image width if needed \n",
    "    'height' : 1 #Default=1. Can be used to stretch image width if needed \n",
    "}\n",
    "\n",
    "#Load image to allow cropping.  Returned item 'crop' is a HoloViews stream object.\n",
    "image,crop,video_dict=lt.LoadAndCrop(video_dict,stretch,cropmethod='Box')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Define Reference Frame for Location Tracking\n",
    "For location tracking to work, view of box without animal must be provided.  Below there are two ways to do this.  **Option 1** provides a method to remove the animal from the video.  This option works well provided the animal doesn't stay in the same location for >50% of the session. Alternatively, with **Option 2**, the user can simply define a video file in the same folder that doesn't have in animal in it.  Option 1 is generally recormmended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Create reference frame by removing animal from video\n",
    "The following code takes 100 random frames across the session and creates an average of them by taking the median for each pixel.  This will remove influence of animal on any given frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=150\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "#Create Reference Frame\n",
    "reference = lt.Reference(video_dict,crop,num_frames=100) \n",
    "\n",
    "#Display Reference Frame\n",
    "image = hv.Image((np.arange(reference.shape[1]), np.arange(reference.shape[0]), reference))\n",
    "image.opts(width=int(reference.shape[1]*stretch['width']),\n",
    "           height=int(reference.shape[0]*stretch['height']),\n",
    "           invert_yaxis=True,cmap='gray',toolbar='below',\n",
    "           title=\"Reference Frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - User specifies video of empty box\n",
    "The following code allows the user to specify a different file.  Notably, an average is still taken of multiple frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "#User selects file\n",
    "video_dict['altfile'] = 'SF2-0training5session2-2Merged.avi' #specify filename of video in dpath directory (e.g. 'Video1.mpg')\n",
    "\n",
    "#Create Reference Frame\n",
    "reference = lt.Reference(video_dict,crop,num_frames=100) \n",
    "\n",
    "#Display Reference Frame\n",
    "image = hv.Image((np.arange(reference.shape[1]), np.arange(reference.shape[0]), reference))\n",
    "image.opts(width=int(reference.shape[1]*stretch['width']),\n",
    "           height=int(reference.shape[0]*stretch['height']),\n",
    "           invert_yaxis=True,cmap='gray',toolbar='below',\n",
    "           title=\"Reference Frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Track Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Set Location Tracking Parameters\n",
    "Location tracking examines the deviance of each frame in a video from the reference frame on a pixel by pixel basis.  For each frame it calculates the center of mass for these differences (COM) to define the center of the animal.  \n",
    "\n",
    "In order to improve accuracy, the parameter loc_thresh is used to remove the influence of pixels that are minimally different from the reference frame.  For each frame relative to the reference frame, the distribution of absolute difference values across pixels is examined and only values above loc_thresh percentile are considered.  I have been using 99 and this works well.  Values can range from 0-100 and floating point numbers are accepted.\n",
    "\n",
    "The parameters: use_window, window, and window_weight are employed to reduce the chance that any objects other than the animal  that might enter the frame (e.g. the hand of the experimenter) influence location tracking.  For each frame, a square window with the animal's position on the prior frame at its center is given more weight when searching for it's location (because an animal presumably can't move far in a fraction of a second).  When window_weight is set to 1, pixels outside of the window are not considered at all; at 0, they are given equal weight.  Notably, setting a high value (e.g. 0.9) should allow the program to more rapidly find the animal if by chance it moves out of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_params = {\n",
    "    'loc_thresh' : 99,\n",
    "    'use_window' : True, #True/False.  Will window surrounding prior location be imposed?\n",
    "    'window_size' : 100, #The length of one side of square window, in pixels.  \n",
    "    'window_weight' : .95, #0-1 scale, where 1 is maximal weight of window surrounding prior locaiton.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Display Examples of Location Tracking to Confirm Threshold\n",
    "In order to confirm threshold is working a subset of images is analyzed and displayed using the selected loc_thresh.  The original image is displayed on top and the difference values are presented below.  The center of mass (COM) is pinpointed on images.  Notably, because individual frames are used, window settings are not applicable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 50\n",
    "examples = 4 #number of examples to display\n",
    "\n",
    "images=lt.LocationThresh_View(examples,video_dict,reference,crop,tracking_params,stretch)\n",
    "images.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Track Location and Save Results to .csv File\n",
    "For each frame the location of the animal's center of mass is recorded in x,y coordinates.  Frame-by-frame distance is also calculated in pixel units.  This data is returned in a Pandas dataframe with columns: frame, x, y, dist.  Data is saved to a .csv in the same folder as the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=lt.TrackLocation(video_dict,tracking_params,reference,crop)\n",
    "\n",
    "#Set output name and save\n",
    "location.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_LocationOutput.csv')\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Display Animal Distance/Location Across Session\n",
    "Below, the animals distance and location across the is plotted.  Smooth traces are expected in the case where they animal is tracked consistently across the session.  Trace of where animal was on each frame is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "#Plot Distance Across Session\n",
    "w, h = 450,200 #specify width and height of plot\n",
    "dist_plot = hv.Curve((location['Frame'],location['Distance']),'Frame','Pixel Distance').opts(\n",
    "    height=h,width=w,color='red',title=\"Distance\",toolbar=\"below\")\n",
    "\n",
    "#Plot Heat Map\n",
    "image = hv.Image((np.arange(reference.shape[1]), np.arange(reference.shape[0]), reference)).opts(\n",
    "    width=int(reference.shape[1]*stretch['width']),\n",
    "    height=int(reference.shape[0]*stretch['height']),\n",
    "    invert_yaxis=True,cmap='gray',toolbar='below',\n",
    "    title=\"Motion Trace\")\n",
    "points = hv.Scatter(np.array([location['X'],location['Y']]).T).opts(color='red',alpha=.1,size=3)\n",
    "tracks = image*points\n",
    "\n",
    "(dist_plot+tracks).cols(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. (Optional) Analyze Activity of Animal in Region of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. User Supplies Names of Regions of Interest.  \n",
    "Although the example only shows two, you can input as many as you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names = [\"dark\",\"light\"] # e.g. region_names = [\"Left\",\"Right\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Use Interactive Plot to Define Regions of Interest.  Supports Polygons. \n",
    "After running cell below, draw regions of interest on presented image in the order you provided them.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell.\n",
    "\n",
    "***Note*** that there are no problems if regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "plot,poly_stream = lt.ROI_plot(reference,region_names,stretch)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. Define ROI Containing Animal for Each Frame and Save Output File as .csv\n",
    "The cell below takes regions of interest provided above and figures out where animal is on each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = lt.ROI_Location(reference,poly_stream,region_names,location)\n",
    "\n",
    "#Set output name and save\n",
    "location.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_LocationOutput.csv')\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. (Optional) Create Binned Summary Report and Save\n",
    "The code below allows the user to either save a csv containing summary data for user-defined bins (e.g. proportion of time in each region and distance travelled for each minute) or a session-wide average. \n",
    "\n",
    "***If you only want a session avg***, set `bin_dict = None`\n",
    "\n",
    "***If you are not using ROIs***, in the code below, set value of 'region_names' within function to None: `region_names=None`.  Otherwise, keep `region_names=region_names`\n",
    "\n",
    "To specify bins, set bin_dict using the following notation, where start and stop represent time in seconds:\n",
    "\n",
    "```\n",
    "bin_dict = {\n",
    "    'BinName1': (start, stop),\n",
    "    'BinName2': (start, stop),\n",
    "    'BinName3': (start, stop),\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Wanna get fancy?  Check out how to use Python dictionary comprehensions to make 20, 30 second bins, using 1 line of code:\n",
    "\n",
    "`bin_dict = {x:(x*30,(x+1)*30) for x in range(20)}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dict = {\n",
    "    1: (0, 30),\n",
    "    2: (30, 60),\n",
    "    3: (60, 90),\n",
    "    4: (90, 120),\n",
    "    5: (120, 150),\n",
    "}\n",
    "\n",
    "summary = lt.Summarize_Location(location, video_dict, bin_dict=bin_dict, region_names=region_names)\n",
    "summary.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_SummaryStats.csv')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. (Optional) View Video of Tracking\n",
    "The code below allows the user to play back a section of the video, with the center of mass of the animal marked, to confirm tracking.  User defines the start and end frames of the section they would like to watch.  Of note, if the user set the start frame for location tracking to something other than 0 in Step 2, the fraame start and end here will be relative to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video parameters\n",
    "display_dict = {\n",
    "    'start' : 300, #start point of video segment in frames.  0 if beginning of video.\n",
    "    'stop' : 900, #end point of video segment in frames.  qqqqqqthis is NOT the duration of the segment\n",
    "    'save_video' : False #Option to save video if desired.  Currently will be saved at 20 fps even if video is something else\n",
    "}\n",
    "\n",
    "#Call function to play video\n",
    "lt.PlayVideo(video_dict,display_dict,crop,location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
