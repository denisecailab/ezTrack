{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The following code was designed in order to track the location of a single animal across the course of a session.  After initally loading in the video, the user is able to crop the video frame using a rectangle selection tool.  A background reference frame is then specified, either by taking a median of several frames in the video, or by the user providing a short video of the same environment without an animal in it.  By comparing each frame in the video to the reference frame, the location of the animal can be tracked.  It is imperative that the reference frame of the video is not shifted from the actual video.  For this reason, if a separate video is supplied, it is best that it be acquired on the same day as the behavioral recording.  The animal's center of mass, in x,y coordinates, is then recorded, as well as the distance in pixels that the animal moves from one frame to the next. Lastly, the user can specify regions of interest in the frame (e.g. left, right) using a polygon drawing tool and record for each frame whether the animal is in the region of interest.  Options for summarizing the data are also provided. \n",
    "\n",
    "### Package Requirements\n",
    "Please see instructions under repository README for package requirements and install instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Load Necessary Packages\n",
    "The following code loads neccessary packages and need not be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import LocationTracking_Functions as lt\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. User Sets Directory and File Information, and Specifies ROI Names, if any.\n",
    "Below the user sets the directory file path (path of the folder where the video file is), the file name of the video to be analyzed, the frame rate of the video, the frame on which the analysis is to begin (0 is the first frame), and the last frame to be analyzed (set `'end' : None` if processing entire video).  Additionally, the user defines the names of any regions of interest.  Any number of regions of interst can be used.  If no regions of interest are to be used, set `region_names = None`.\n",
    "\n",
    "***Windows Users:*** Place an 'r' in front directory path (e.g. r\"zp\\Videos\") to avoid mishandling of forward slashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {\n",
    "    'dpath' : \"/Users/zachpennington/Desktop/Videos/\", # directory containing file\n",
    "    'file' : 'Test.wmv', #filename with extension\n",
    "    'fps' : 30, #frames per second\n",
    "    'start' : 0, #frame at which to start. 0-based\n",
    "    'end' : None #frame at which to end.  set to None if processing whole video.\n",
    "}\n",
    "\n",
    "#Specify ROIs.  Although example only shows 2, any number is allowed\n",
    "region_names = None # e.g. region_names = [\"Left\",\"Right\"], or region_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Load Video and Crop Frame if Desired\n",
    "To crop video frame, after running code below, select box selection tool below image (square with a plus sign).  To start drawing region to be included in analyis, double click image.  Double click again to finalize region.  If you decide to change region, it is best to rerun this cell and subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "#stretch image size if desired. only necessary if video width/height are odd\n",
    "#which can make image hard to see.\n",
    "stretch ={\n",
    "    'width' : 1 , #Default=1. Can be used to stretch image width if needed \n",
    "    'height' : 1 #Default=1. Can be used to stretch image width if needed \n",
    "}\n",
    "\n",
    "#Load image to allow cropping.  Returned item 'crop' is a HoloViews stream object.\n",
    "image,crop,video_dict=lt.LoadAndCrop(video_dict,stretch,cropmethod='Box')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Define Reference Frame for Location Tracking.\n",
    "For location tracking to work, view of box without animal must be provided.  Below there are two ways to do this.  **Option 1** provides a method to remove the animal from the video.  This option works well provided the animal doesn't stay in the same location for >50% of the session. Alternatively, with **Option 2**, the user can simply define a video file in the same folder that doesn't have in animal in it.  Option 1 is generally recormmended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Create reference frame by removing animal from video\n",
    "The following code takes 100 random frames across the session and creates an average of them by taking the median for each pixel.  This will remove influence of animal on any given frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "#Create Reference Frame and display\n",
    "reference = lt.Reference(video_dict,crop,num_frames=100) \n",
    "image = hv.Image((np.arange(reference.shape[1]), np.arange(reference.shape[0]), reference)).opts(width=int(reference.shape[1]*stretch['width']),\n",
    "           height=int(reference.shape[0]*stretch['height']),\n",
    "           invert_yaxis=True,cmap='gray',toolbar='below',\n",
    "           title=\"Reference Frame\")\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - User specifies video of empty box\n",
    "The following code allows the user to specify a different file.  Set video_dict['altfile'] to the alternative filename.  Notably, an average is still taken of multiple frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "#User selects file\n",
    "video_dict['altfile'] = 'EmptyBox.avi' #specify filename of video in dpath directory (e.g. 'Video1.mpg')\n",
    "\n",
    "#Create Reference Frame\n",
    "reference = lt.Reference(video_dict,crop,num_frames=100) \n",
    "image = hv.Image((np.arange(reference.shape[1]), np.arange(reference.shape[0]), reference)).opts(width=int(reference.shape[1]*stretch['width']),\n",
    "           height=int(reference.shape[0]*stretch['height']),\n",
    "           invert_yaxis=True,cmap='gray',toolbar='below',\n",
    "           title=\"Reference Frame\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (Optional) Use Interactive Plot to Define Regions of Interest.  \n",
    "**Do not run if region_names is set to None.**\n",
    "\n",
    "After running cell below, draw regions of interest on presented image in the order you provided them.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell.\n",
    "\n",
    "***Note*** that there are no problems if regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "#Select output size if image is too small/large.  Code above must be first line in cell and dictates overall size\n",
    "#of image, where 100 is standard.  \n",
    "\n",
    "plot,poly_stream = lt.ROI_plot(reference,region_names,stretch)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Track Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Set Location Tracking Parameters\n",
    "Location tracking examines the deviance of each frame in a video from the reference frame on a pixel by pixel basis.  For each frame it calculates the center of mass for these differences (COM) to define the center of the animal.  \n",
    "\n",
    "In order to improve accuracy, the parameter loc_thresh is used to remove the influence of pixels that are minimally different from the reference frame.  For each frame relative to the reference frame, the distribution of absolute difference values across pixels is examined and only values above loc_thresh percentile are considered.  We have been using 99-99.5 and this works well.  Values can range from 0-100 and floating point numbers are accepted.\n",
    "\n",
    "The parameters: use_window, window_size, and window_weight are employed to reduce the chance that any objects other than the animal  that might enter the frame (e.g. the hand of the experimenter) influence location tracking.  For each frame, a square window with the animal's position on the prior frame at its center is given more weight when searching for it's location (because an animal presumably can't move far in a fraction of a second).  When window_weight is set to 1, pixels outside of the window are not considered at all; at 0, they are given equal weight.  Notably, setting a high value that is still not equal to 1 (e.g. 0.9) should allow the program to more rapidly find the animal if by chance it moves out of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_params = {\n",
    "    'loc_thresh' : 99.5, #Percentile of difference values below which difference values are set to 0.\n",
    "    'use_window' : True, #True/False.  Will window surrounding prior location be imposed?\n",
    "    'window_size' : 100, #The length of one side of square window, in pixels.  \n",
    "    'window_weight' : .9, #0-1 scale, where 1 is maximal weight of window surrounding prior locaiton.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Display Examples of Location Tracking to Confirm Threshold\n",
    "In order to confirm threshold is working, a subset of images is analyzed and displayed using the selected loc_thresh.  The original image is displayed on top and the difference values are presented below.  The center of mass (COM) is pinpointed on images.  Notably, because individual frames are used, window settings are not applicable here.  Because of this, actual tracking in video is likely to be better.\n",
    "\n",
    "The user can change the number examples below as they see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 50\n",
    "examples = 5 #number of examples to display\n",
    "\n",
    "images=lt.LocationThresh_View(examples,video_dict,reference,crop,tracking_params,stretch)\n",
    "images.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. Track Location and Save Results to .csv File\n",
    "For each frame the location of the animal's center of mass is recorded in x,y coordinates.  If ROIs are supplied, for each frame it is determined whether the animal is in each of the ROIs.  Frame-by-frame distance is also calculated in pixel units.  This data is returned in a Pandas dataframe with columns: frame, x, y, dist, and whether the animal is in each ROI specified (True/False).  Data is saved to a .csv in the same folder as the video.  First 5 rows of data are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track location\n",
    "location=lt.TrackLocation(video_dict,tracking_params,reference,crop)\n",
    "if region_names != None: \n",
    "    location = lt.ROI_Location(reference,poly_stream,region_names,location) #add ROI info\n",
    "location.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_LocationOutput.csv')\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d. Display Animal Distance/Location Across Session\n",
    "Below, the animals distance and location across the video is plotted.  Smooth traces are expected in the case where the animal is tracked consistently across the session.  Trace of where animal was on each frame is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "#Plot Distance Across Session\n",
    "w, h = 800,200 #specify width and height of plot\n",
    "dist_plot = hv.Curve((location['Frame'],location['Distance']),'Frame','Pixel Distance').opts(\n",
    "    height=h,width=w,color='red',title=\"X Location Across Session\",toolbar=\"below\")\n",
    "\n",
    "#Plot Trace of Animal Across Session\n",
    "image = hv.Image((np.arange(reference.shape[1]), np.arange(reference.shape[0]), reference)).opts(\n",
    "    width=int(reference.shape[1]*stretch['width']),\n",
    "    height=int(reference.shape[0]*stretch['height']),\n",
    "    invert_yaxis=True,cmap='gray',toolbar='below',\n",
    "    title=\"Motion Trace\")\n",
    "points = hv.Scatter(np.array([location['X'],location['Y']]).T).opts(color='red',alpha=.8,size=3)\n",
    "tracks = image*points\n",
    "(dist_plot+tracks).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. (Optional) Create Binned Summary Report and Save\n",
    "The code below allows the user to either save a csv containing summary data for user-defined bins (e.g. proportion of time in each region and distance travelled for each minute) or a session-wide average. \n",
    "\n",
    "***If you only want a session avg***, set `bin_dict = None`\n",
    "\n",
    "***If you are not using ROIs***, in the code below, set value of 'region_names' within function to None: `region_names=None`.  Otherwise, keep `region_names=region_names`\n",
    "\n",
    "To specify bins, set bin_dict using the following notation, where start and stop represent time in seconds:\n",
    "\n",
    "```\n",
    "bin_dict = {\n",
    "    'BinName1': (start, stop),\n",
    "    'BinName2': (start, stop),\n",
    "    'BinName3': (start, stop),\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Wanna get fancy?  Check out how to use Python dictionary comprehensions to make 20, 30 second bins, using 1 line of code:\n",
    "\n",
    "`bin_dict = {x:(x*30,(x+1)*30) for x in range(20)}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dict = {\n",
    "    '1':(0,300),\n",
    "    '2':(300,600),\n",
    "    '3':(600,900)\n",
    "}\n",
    "\n",
    "summary = lt.Summarize_Location(location, video_dict, bin_dict=bin_dict, region_names=region_names)\n",
    "summary.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_SummaryStats.csv')\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. (Optional) View Video of Tracking\n",
    "**Note** that tracking must be done before this (Step 6c). \n",
    "The code below allows the user to play back a section of the analyze video, with the center of mass of the animal marked, to confirm tracking.  User defines the start and end frames of the section they would like to watch.  Of note, if the user set the start frame for location tracking to something other than 0 in Step 2, the fraame start and end here will be relative to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video parameters\n",
    "display_dict = {\n",
    "    'start' : 0, #start point of video segment in frames.  0 if beginning of video.\n",
    "    'stop' : 500, #end point of video segment in frames.  this is NOT the duration of the segment\n",
    "    'save_video' : False #Option to save video if desired.  Currently will be saved at 20 fps even if video is something else\n",
    "}\n",
    "\n",
    "#Call function to play video\n",
    "lt.PlayVideo(video_dict,display_dict,crop,location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
